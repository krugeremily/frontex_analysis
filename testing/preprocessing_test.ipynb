{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing code to extract text from pdfs, make all lowercase, remove stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(pdf_path):\n",
    "    text = ''\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract text content from the page\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + '\\n'  # Append extracted text with a newline\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove puncuation\n",
    "    pattern1 = r\"[^\\w\\s']\"\n",
    "    pattern2 = '\\n'\n",
    "    text = re.sub(pattern1, '', text)\n",
    "    text = re.sub(pattern2, ' ', text)\n",
    "\n",
    "    #tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Convert tokens back to text\n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "def process_pdfs(folder_path, output_folder):\n",
    "    pdf_files = [f for f in os.listdir(folder_path)[:2] if f.endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        \n",
    "        # Extract text from PDF using PyPDF2\n",
    "        extracted_text = extract_text(pdf_path)\n",
    "        \n",
    "        # Preprocess extracted text\n",
    "        preprocessed_text = preprocess_text(extracted_text)\n",
    "        \n",
    "        # Save preprocessed text to a new text file\n",
    "        output_file_path = os.path.join(output_folder, os.path.splitext(pdf_file)[0] + '.txt')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(preprocessed_text)\n",
    "        \n",
    "        print('Preprocessing for doc done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing for doc done\n",
      "Preprocessing for doc done\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "folder_path = '../data/raw_data'\n",
    "output_folder = '../data/preprocessed_data'\n",
    "os.makedirs('../data/preprocessed_data', exist_ok=True)\n",
    "\n",
    "process_pdfs(folder_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "with pdfplumber.open('/Users/emilykruger/Documents/GitHub/frontex_analysis/data/raw_data/2023.11.16_frontex-general-industry-days-innovation-for-border-and-coast-guard-functions.pdf') as pdf:\n",
    "    for page in pdf.pages:\n",
    "        # Extract text content from the page\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text  # Append extracted text with a newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frontex General Industry Days: Innovation for\\nborder and coast guard functions\\n2023-11-16\\nJoin us on 6 and 7 December to contribute to innovative solutions for border and coast guard\\nfunctions. Frontex’s next general Industry Days will put innovation in the spotlight to reflect the core\\nrole of technology in European Integrated Border Management.\\nFrontex would like to invite industry representatives to demonstrate how innovation could support\\nborder and coast guard functions. Over the course of a two-day programme, 16 industry\\nrepresentatives will present their latest approaches, technologies, and solutions (whether already\\navailable on the market or under development), which can benefit border management activities at\\nthe EU’s external borders and within the EU area, in respect of EU regulations.\\nThe first day of the event will have a broad scope, it will be dedicated to innovative solutions in\\nsupport of law enforcement activities regarding border management.\\nThe second day will focus on remote sensing technologies for electromagnetic signatures\\nrecognition and state-of-the-art sensors for surveillance purposes (land based, shipborne,\\nairborne, mobile, etc.).\\nThese areas of focus should not be considered as exhaustive; other technological solutions will be\\nconsidered if relevant for border management.\\nPractical information\\nWhat are Industry Days?\\nIndustry Days are a platform to interact and engage with experts from the European Border and Coast\\nGuard community. The goal is to create a forum where participants can freely discuss the proposed\\ntechnological innovations.\\nFrontex - European Border and Coast Guard Agency\\nwww.frontex.europa.eu | Pl. Europejski 6, 00-844 Warsaw, Poland | Tel. +48 22 205 95 00 | Fax +48 22 205 95 01The Industry Days will be held in an online format which will host engaging presentations and\\ndynamic discussions, alongside an interactive digital exhibition platform. Each participating entity will\\nbe granted a 40-minute slot; during this time a presentation, Q&A session, and collaborative\\ndiscussions will be held.\\nWho can participate and how to register?\\nIf your organisation is at the forefront of these transformative technological advancements and you\\nwish to present your innovation, we invite you to register.\\nFollow this registration link to sign-up for the upcoming Industry Days. Please provide an overview\\nof the technology you wish to showcase. Registration will be open until 27 November.\\nAttendance is by invitation only; more information will be communicated separately to the selected\\napplicants. Technological solutions must respect the legal limitations resulting from the European\\nUnion data protection rules and human rights provisions.\\nHow will the submissions be evaluated?\\nAll submissions will be evaluated internally, ensuring alignment with the needs of the European\\nBorder and Coast Guard community while guaranteeing the fair and transparent treatment of all\\nparticipants. Every significant contribution will be displayed on our dedicated online platform.\\nError! No text of specified style in document. 2/02'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frontex european border coast guard agency wwwfrontexeuropaeu pl europejski 6 00 844 warsaw poland tel 48 22 205 95 00 fax 48 22 205 95 01 frontex general industry days innovation border coast guard functions 2023 1116 join us 6 7 december contribute innovative solutions border coast guard functions frontexs next general industry days put innovation th e spotlight reflect core role technology european integrated border management frontex would like invite industry representatives demonstrate innovation could support border coast guard functions course two day pr ogramme 16 industry representatives present latest approaches technologies solutions whether already available market development benefit border management activities eus external borders within th e eu area respect eu regulations first day event broad scope dedicated innovative solutions support law enforcement activities regarding border management second day focus remote sensing tec hnologies electromagnetic signatures recognition state oftheart sensors surveillance purposes land based shipborne airborne mobile etc areas focus considered exhaustive technological solutions considered relevant border management practical information industry days industry days platform interact engage experts european border coast guard community goal create forum participants freely discuss proposed technological innovations error text specified style document 202 industry days held online format host engaging presentations dynamic discussions alongside interactive digital exhibition platform participating entity granted 40 minute slot time presentation qa session collabor ative discussions held participate register organisation forefront transformative technological advancements wish present innovation invite register follow registration link sign upcoming industry days please provide overview technology wish showcase registration open 27 november attendance invitation information communicated separately selected applicants technological solutions must respect legal limitations resulting european union data protection rules human rights provisions submissions evaluated submissions evaluated internally ensuring alignme nt needs european border coast guard community guaranteeing fair transparent treatment participants every significant contribution displayed dedicated online platform'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corupus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create corupus\n",
    "# def create_corpus(folder_path):\n",
    "#     files = os.listdir(folder_path)\n",
    "#     corpus = []\n",
    "\n",
    "#     for file in files[:2]:\n",
    "#         if file.endswith('.txt'):\n",
    "#             with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "#                 text = f.read()\n",
    "#                 corpus.append((file, text))  # Store file name and text content as tuple\n",
    "    \n",
    "#     return corpus\n",
    "\n",
    "\n",
    "# corpus = create_corpus(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_tokenize_documents(folder_path):\n",
    "    tokenized_corpus = []\n",
    "\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
    "                tokenized_corpus.append(tokens)\n",
    "    \n",
    "    return tokenized_corpus\n",
    "\n",
    "folder_path = '../data/preprocessed_data'\n",
    "corpus = load_and_tokenize_documents(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def calculate_tfidf(tokenized_corpus):\n",
    "    # Prepare documents and file names from the tokenized corpus\n",
    "    documents = [' '.join(doc) for doc in tokenized_corpus]  # Join tokens into space-separated strings\n",
    "    file_names = os.listdir(folder_path)[:2]\n",
    "    \n",
    "    # Calculate TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a list of dictionaries containing filename and top 5 TF-IDF words\n",
    "    result = []\n",
    "    for i, filename in enumerate(file_names):\n",
    "        tfidf_scores = list(zip(feature_names, tfidf_matrix[i].toarray().flatten()))\n",
    "        tfidf_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_tfidf_words = [word for word, score in tfidf_scores[:5]]\n",
    "        result.append({'Filename': filename, 'Top 5 TF-IDF Words': top_tfidf_words})\n",
    "\n",
    "    return result\n",
    "\n",
    "# Calculate TF-IDF matrix, feature names, and file names\n",
    "result = calculate_tfidf(corpus)\n",
    "\n",
    "# Create DataFrame from the result list of dictionaries\n",
    "results = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Top 5 TF-IDF Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fran_q1_2011.txt</td>\n",
       "      <td>[2011, q1, detections, eu, 2010]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afic_2017.txt</td>\n",
       "      <td>[afic, 2017, niger, libya, migrants]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Filename                    Top 5 TF-IDF Words\n",
       "0  fran_q1_2011.txt      [2011, q1, detections, eu, 2010]\n",
       "1     afic_2017.txt  [afic, 2017, niger, libya, migrants]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/emilykruger/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    # Analyze sentiment using NLTK Vader\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sid.polarity_scores(text)['compound']\n",
    "    return sentiment_score\n",
    "\n",
    "# Example usage (for each document in the corpus):\n",
    "sentiment_scores = [analyze_sentiment(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['sentiment score'] = sentiment_scores[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sentiment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=corpus, vector_size=300, window=5, min_count=1, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'migration': [('irregular', 0.6413043141365051), ('migra', 0.5688313841819763), ('migratory', 0.556184709072113), ('gration', 0.5402964949607849), ('irregu', 0.5318630337715149)]\n",
      "Similar words to 'migrat': [('overcrowded', 0.6400548815727234), ('room', 0.6280505061149597), ('rently', 0.6272342205047607), ('boss', 0.6161612868309021), ('parallel', 0.6138434410095215)]\n",
      "Similar words to 'refugee': [('automatic', 0.6764104962348938), ('ognised', 0.6716601252555847), ('transregional', 0.6708460450172424), ('70migrants', 0.6699479222297668), ('strictions', 0.6603562235832214)]\n"
     ]
    }
   ],
   "source": [
    "# Find similar words to a keyword using the fine-tuned model\n",
    "similar_words_migration = model.wv.most_similar('migration', topn=5)\n",
    "print(\"Similar words to 'migration':\", similar_words_migration)\n",
    "# Find similar words to a keyword using the fine-tuned model\n",
    "similar_words_migrat = model.wv.most_similar('migrant', topn=5)\n",
    "print(\"Similar words to 'migrat':\", similar_words_migrat)\n",
    "# Find similar words to a keyword using the fine-tuned model\n",
    "similar_words_refugee = model.wv.most_similar('refugee', topn=5)\n",
    "print(\"Similar words to 'refugee':\", similar_words_refugee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../model/word2vec_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frontex_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
